{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5af8a2c0-45fe-4d13-bebd-0dca87a7b71f",
      "metadata": {
        "id": "5af8a2c0-45fe-4d13-bebd-0dca87a7b71f"
      },
      "source": [
        "# Library import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "CYmQAhDfU-jn",
      "metadata": {
        "id": "CYmQAhDfU-jn"
      },
      "outputs": [],
      "source": [
        "# !pip install opencv-python\n",
        "# !pip install timm\n",
        "# !pip install torch torchvision torchaudio\n",
        "# !pip install numpy\n",
        "# !pip install pandas\n",
        "# !pip install albumentations\n",
        "# !pip install tqdm\n",
        "# !pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "a4c611c8-2226-433c-bf5f-343cc0b094af",
      "metadata": {
        "id": "a4c611c8-2226-433c-bf5f-343cc0b094af"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Tuple, Any, Callable, List, Optional, Union\n",
        "\n",
        "import cv2\n",
        "import timm\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import albumentations as A\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import models, datasets, transforms\n",
        "from torchvision.transforms import functional as F  # 추가\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from PIL import Image, ImageFilter  # 추가\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import functional as TF  # torchvision의 functional을 TF로 임포트\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2d69e6a-a719-4a97-92ca-6354c873313f",
      "metadata": {
        "id": "a2d69e6a-a719-4a97-92ca-6354c873313f"
      },
      "source": [
        "# Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "56f97229-e29f-479d-abab-0db8219d1803",
      "metadata": {
        "id": "56f97229-e29f-479d-abab-0db8219d1803"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        root_dir: str,\n",
        "        info_df: pd.DataFrame,\n",
        "        transform: Callable,\n",
        "        is_inference: bool = False\n",
        "    ):\n",
        "        # 데이터셋의 기본 경로, 이미지 변환 방법, 이미지 경로 및 레이블을 초기화합니다.\n",
        "        self.root_dir = root_dir  # 이미지 파일들이 저장된 기본 디렉토리\n",
        "        self.transform = transform  # 이미지에 적용될 변환 처리\n",
        "        self.is_inference = is_inference # 추론인지 확인\n",
        "        self.image_paths = info_df['image_path'].tolist()  # 이미지 파일 경로 목록\n",
        "\n",
        "        if not self.is_inference:\n",
        "            self.targets = info_df['target'].tolist()  # 각 이미지에 대한 레이블 목록\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        # 데이터셋의 총 이미지 수를 반환합니다.\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Union[Tuple[torch.Tensor, int], torch.Tensor]:\n",
        "        # 주어진 인덱스에 해당하는 이미지를 로드하고 변환을 적용한 후, 이미지와 레이블을 반환합니다.\n",
        "        img_path = os.path.join(self.root_dir, self.image_paths[index])  # 이미지 경로 조합\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)  # 이미지를 BGR 컬러 포맷의 numpy array로 읽어옵니다.\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # BGR 포맷을 RGB 포맷으로 변환합니다.\n",
        "        image = self.transform(image)  # 설정된 이미지 변환을 적용합니다.\n",
        "\n",
        "        if self.is_inference:\n",
        "            return image\n",
        "        else:\n",
        "            target = self.targets[index]  # 해당 이미지의 레이블\n",
        "            return image, target  # 변환된 이미지와 레이블을 튜플 형태로 반환합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c07d2d0-9585-45ce-8ece-4f69b98f6dd4",
      "metadata": {
        "id": "6c07d2d0-9585-45ce-8ece-4f69b98f6dd4"
      },
      "source": [
        "# Transform Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "9b1855c1-cf13-476d-aabd-d78e9e082ddf",
      "metadata": {
        "id": "9b1855c1-cf13-476d-aabd-d78e9e082ddf"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class TorchvisionTransform:\n",
        "    def __init__(self, is_train: bool = True):\n",
        "        # 공통 변환 설정: 이미지 리사이즈, 텐서 변환, 정규화\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),  # 이미지를 224x224 크기로 리사이즈\n",
        "            transforms.ToTensor(),          # 이미지를 PyTorch 텐서로 변환\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],  # 이미지 채널별 평균값\n",
        "                std=[0.229, 0.224, 0.225]    # 이미지 채널별 표준편차\n",
        "            )\n",
        "        ])\n",
        "\n",
        "    def __call__(self, image: np.ndarray) -> torch.Tensor:\n",
        "        image = Image.fromarray(image)  # numpy 배열을 PIL 이미지로 변환\n",
        "        transformed = self.transform(image)  # 설정된 변환을 적용\n",
        "        return transformed  # 변환된 이미지 반환\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "0a683988-0f73-4e43-907b-0d5209550abb",
      "metadata": {
        "id": "0a683988-0f73-4e43-907b-0d5209550abb"
      },
      "outputs": [],
      "source": [
        "class AlbumentationsTransform:\n",
        "    def __init__(self, is_train: bool = True):\n",
        "        # 공통 변환 설정: 이미지 리사이즈, 정규화, 텐서 변환\n",
        "        common_transforms = [\n",
        "            A.Resize(224, 224),  # 이미지를 224x224 크기로 리사이즈\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # 정규화\n",
        "            ToTensorV2()  # albumentations에서 제공하는 PyTorch 텐서 변환\n",
        "        ]\n",
        "\n",
        "        # if is_train:\n",
        "        #     # 훈련용 변환: 랜덤 수평 뒤집기, 랜덤 회전, 랜덤 밝기 및 대비 조정 추가\n",
        "        #     self.transform = A.Compose(\n",
        "        #         [\n",
        "        #             A.HorizontalFlip(p=0.5),  # 50% 확률로 이미지를 수평 뒤집기\n",
        "        #             A.Rotate(limit=15),  # 최대 15도 회전\n",
        "        #             A.RandomBrightnessContrast(p=0.2),  # 밝기 및 대비 무작위 조정\n",
        "        #         ] + common_transforms\n",
        "        #     )\n",
        "        # else:\n",
        "        #     # 검증/테스트용 변환: 공통 변환만 적용\n",
        "        #     self.transform = A.Compose(common_transforms)\n",
        "\n",
        "    def __call__(self, image) -> torch.Tensor:\n",
        "        # 이미지가 NumPy 배열인지 확인\n",
        "        if not isinstance(image, np.ndarray):\n",
        "            raise TypeError(\"Image should be a NumPy array (OpenCV format).\")\n",
        "\n",
        "        # 이미지에 변환 적용 및 결과 반환\n",
        "        transformed = self.transform(image=image)  # 이미지에 설정된 변환을 적용\n",
        "\n",
        "        return transformed['image']  # 변환된 이미지의 텐서를 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "e82f3416-86f2-430f-9260-d23904e757e4",
      "metadata": {
        "id": "e82f3416-86f2-430f-9260-d23904e757e4"
      },
      "outputs": [],
      "source": [
        "class TransformSelector:\n",
        "    \"\"\"\n",
        "    이미지 변환 라이브러리를 선택하기 위한 클래스.\n",
        "    \"\"\"\n",
        "    def __init__(self, transform_type: str):\n",
        "\n",
        "        # 지원하는 변환 라이브러리인지 확인\n",
        "        if transform_type in [\"torchvision\", \"albumentations\"]:\n",
        "            self.transform_type = transform_type\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Unknown transformation library specified.\")\n",
        "\n",
        "    def get_transform(self, is_train: bool):\n",
        "\n",
        "        # 선택된 라이브러리에 따라 적절한 변환 객체를 생성\n",
        "        if self.transform_type == 'torchvision':\n",
        "            transform = TorchvisionTransform(is_train=is_train)\n",
        "\n",
        "        elif self.transform_type == 'albumentations':\n",
        "            transform = AlbumentationsTransform(is_train=is_train)\n",
        "\n",
        "        return transform"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c938bcb2-9257-49cb-8d05-dd4a7bb25665",
      "metadata": {
        "id": "c938bcb2-9257-49cb-8d05-dd4a7bb25665"
      },
      "source": [
        "# Model Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "f16fb24a-8d34-4ed6-8a33-2d153d12d190",
      "metadata": {
        "id": "f16fb24a-8d34-4ed6-8a33-2d153d12d190"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    간단한 CNN 아키텍처를 정의하는 클래스.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        # 순전파 함수 정의\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "f91493ca-c5c2-4950-916a-cc4304c7ad4a",
      "metadata": {
        "id": "f91493ca-c5c2-4950-916a-cc4304c7ad4a"
      },
      "outputs": [],
      "source": [
        "class TorchvisionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Torchvision에서 제공하는 사전 훈련된 모델을 사용하는 클래스.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        num_classes: int,\n",
        "        pretrained: bool\n",
        "    ):\n",
        "        super(TorchvisionModel, self).__init__()\n",
        "        self.model = models.__dict__[model_name](pretrained=pretrained)\n",
        "\n",
        "        # 모델의 최종 분류기 부분을 사용자 정의 클래스 수에 맞게 조정\n",
        "        if 'fc' in dir(self.model):\n",
        "            num_ftrs = self.model.fc.in_features\n",
        "            self.model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "        elif 'classifier' in dir(self.model):\n",
        "            num_ftrs = self.model.classifier[-1].in_features\n",
        "            self.model.classifier[-1] = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "f28c8e4f-a914-4b12-982e-d4a58863c717",
      "metadata": {
        "id": "f28c8e4f-a914-4b12-982e-d4a58863c717"
      },
      "outputs": [],
      "source": [
        "class TimmModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Timm 라이브러리를 사용하여 다양한 사전 훈련된 모델을 제공하는 클래스.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        num_classes: int,\n",
        "        pretrained: bool\n",
        "    ):\n",
        "        super(TimmModel, self).__init__()\n",
        "        self.model = timm.create_model(\n",
        "            model_name,\n",
        "            pretrained=pretrained,\n",
        "            num_classes=num_classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "3f2da081-9010-431d-a049-835d7bbea4a5",
      "metadata": {
        "id": "3f2da081-9010-431d-a049-835d7bbea4a5"
      },
      "outputs": [],
      "source": [
        "class ModelSelector:\n",
        "    \"\"\"\n",
        "    사용할 모델 유형을 선택하는 클래스.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_type: str,\n",
        "        num_classes: int,\n",
        "        **kwargs\n",
        "    ):\n",
        "\n",
        "        # 모델 유형에 따라 적절한 모델 객체를 생성\n",
        "        if model_type == 'simple':\n",
        "            self.model = SimpleCNN(num_classes=num_classes)\n",
        "\n",
        "        elif model_type == 'torchvision':\n",
        "            self.model = TorchvisionModel(num_classes=num_classes, **kwargs)\n",
        "\n",
        "        elif model_type == 'timm':\n",
        "            self.model = TimmModel(num_classes=num_classes, **kwargs)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Unknown model type specified.\")\n",
        "\n",
        "    def get_model(self) -> nn.Module:\n",
        "\n",
        "        # 생성된 모델 객체 반환\n",
        "        return self.model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2977c7b-bc39-48f7-8155-ef6b6a03d6f8",
      "metadata": {
        "id": "c2977c7b-bc39-48f7-8155-ef6b6a03d6f8"
      },
      "source": [
        "# Loss Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "97471eb3-a979-4fb3-b976-6c3177c79f76",
      "metadata": {
        "id": "97471eb3-a979-4fb3-b976-6c3177c79f76"
      },
      "outputs": [],
      "source": [
        "# class Loss(nn.Module):\n",
        "#     \"\"\"\n",
        "#     모델의 손실함수를 계산하는 클래스.\n",
        "#     \"\"\"\n",
        "#     def __init__(self):\n",
        "#         super(Loss, self).__init__()\n",
        "#         self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "#     def forward(\n",
        "#         self,\n",
        "#         outputs: torch.Tensor,\n",
        "#         targets: torch.Tensor\n",
        "#     ) -> torch.Tensor:\n",
        "\n",
        "#         return self.loss_fn(outputs, targets)\n",
        "\n",
        "\n",
        "\n",
        "# 6. 손실 함수 수정 (CrossEntropyLoss로 변경)\n",
        "# 기존의 Loss 클래스 대신 CrossEntropyLoss를 사용합니다.\n",
        "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f3e3d21-5ab8-41b0-aa5c-e62ace8dc6a6",
      "metadata": {
        "id": "5f3e3d21-5ab8-41b0-aa5c-e62ace8dc6a6"
      },
      "source": [
        "# Trainer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "6a90c673-6672-4066-a9ec-9975d7842be4",
      "metadata": {
        "id": "6a90c673-6672-4066-a9ec-9975d7842be4"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        device: torch.device,\n",
        "        train_loader: DataLoader,\n",
        "        val_loader: DataLoader,\n",
        "        optimizer: optim.Optimizer,\n",
        "        scheduler: optim.lr_scheduler,\n",
        "        loss_fn: torch.nn.modules.loss._Loss,\n",
        "        epochs: int,\n",
        "        result_path: str\n",
        "    ):\n",
        "        # 클래스 초기화: 모델, 디바이스, 데이터 로더 등 설정\n",
        "        self.model = model  # 훈련할 모델\n",
        "        self.device = device  # 연산을 수행할 디바이스 (CPU or GPU)\n",
        "        self.train_loader = train_loader  # 훈련 데이터 로더\n",
        "        self.val_loader = val_loader  # 검증 데이터 로더\n",
        "        self.optimizer = optimizer  # 최적화 알고리즘\n",
        "        self.scheduler = scheduler # 학습률 스케줄러\n",
        "        self.loss_fn = loss_fn  # 손실 함수\n",
        "        self.epochs = epochs  # 총 훈련 에폭 수\n",
        "        self.result_path = result_path  # 모델 저장 경로\n",
        "        self.best_models = [] # 가장 좋은 상위 3개 모델의 정보를 저장할 리스트\n",
        "        self.lowest_loss = float('inf') # 가장 낮은 Loss를 저장할 변수\n",
        "\n",
        "    def save_model(self, epoch, loss):\n",
        "        # 모델 저장 경로 설정\n",
        "        os.makedirs(self.result_path, exist_ok=True)\n",
        "\n",
        "        # 현재 에폭 모델 저장\n",
        "        current_model_path = os.path.join(self.result_path, f'model_epoch_{epoch}_loss_{loss:.4f}.pt')\n",
        "        torch.save(self.model.state_dict(), current_model_path)\n",
        "\n",
        "        # 최상위 3개 모델 관리\n",
        "        self.best_models.append((loss, epoch, current_model_path))\n",
        "        self.best_models.sort()\n",
        "        if len(self.best_models) > 3:\n",
        "            _, _, path_to_remove = self.best_models.pop(-1)  # 가장 높은 손실 모델 삭제\n",
        "            if os.path.exists(path_to_remove):\n",
        "                os.remove(path_to_remove)\n",
        "\n",
        "        # 가장 낮은 손실의 모델 저장\n",
        "        if loss < self.lowest_loss:\n",
        "            self.lowest_loss = loss\n",
        "            best_model_path = os.path.join(self.result_path, 'best_model.pt')\n",
        "            torch.save(self.model.state_dict(), best_model_path)\n",
        "            print(f\"Save {epoch}epoch result. Loss = {loss:.4f}\")\n",
        "\n",
        "    def train_epoch(self) -> float:\n",
        "        # 한 에폭 동안의 훈련을 진행\n",
        "        self.model.train()\n",
        "\n",
        "        total_loss = 0.0\n",
        "        progress_bar = tqdm(self.train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "        for images, targets in progress_bar:\n",
        "            images, targets = images.to(self.device), targets.to(self.device)\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(images)\n",
        "            loss = self.loss_fn(outputs, targets)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            self.scheduler.step()  # 배치마다 스케줄러 업데이트\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        return total_loss / len(self.train_loader)\n",
        "\n",
        "    def validate(self) -> tuple[float, float]:\n",
        "        # 모델의 검증을 진행\n",
        "        self.model.eval()\n",
        "\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        progress_bar = tqdm(self.val_loader, desc=\"Validating\", leave=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, targets in progress_bar:\n",
        "                images, targets = images.to(self.device), targets.to(self.device)\n",
        "                outputs = self.model(images)\n",
        "                loss = self.loss_fn(outputs, targets)\n",
        "                \n",
        "                # 정확도 계산을 위한 예측값 확인\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct += (predicted == targets).sum().item()\n",
        "                total += targets.size(0)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                progress_bar.set_postfix(loss=loss.item(), accuracy=correct / total)\n",
        "\n",
        "        val_loss = total_loss / len(self.val_loader)\n",
        "        val_accuracy = (correct / total) * 100  # 검증 데이터셋에 대한 정확도\n",
        "\n",
        "        return val_loss, val_accuracy\n",
        "\n",
        "\n",
        "    def train(self) -> None:\n",
        "        # 전체 훈련 과정을 관리\n",
        "        for epoch in range(self.epochs):\n",
        "            print(f\"Epoch {epoch+1}/{self.epochs}\")\n",
        "\n",
        "            train_loss = self.train_epoch()\n",
        "            val_loss, val_accuracy = self.validate()\n",
        "            print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
        "\n",
        "            self.save_model(epoch, val_loss)\n",
        "            # self.scheduler.step()  # 에폭마다 스케줄러 업데이트는 제거합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22f41c09-318a-4f2e-bdca-68d8a07e9938",
      "metadata": {
        "id": "22f41c09-318a-4f2e-bdca-68d8a07e9938"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "698783c4-ac2a-4e66-82aa-637df06ce012",
      "metadata": {
        "id": "698783c4-ac2a-4e66-82aa-637df06ce012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# 학습에 사용할 장비를 선택.\n",
        "# torch라이브러리에서 gpu를 인식할 경우, cuda로 설정.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "76cfe17e-fb14-42e4-84ae-b6773f0b78fb",
      "metadata": {
        "id": "76cfe17e-fb14-42e4-84ae-b6773f0b78fb"
      },
      "outputs": [],
      "source": [
        "# 학습 데이터의 경로와 정보를 가진 파일의 경로를 설정.\n",
        "traindata_dir = \"data/sketch/train\"\n",
        "traindata_info_file = \"/data/ephemeral/home/level1-imageclassification-cv-12/data/sketch/train.csv\"\n",
        "save_result_path = \"result\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "42a4778f-bfd0-4638-8972-f366cd6b0a4f",
      "metadata": {
        "id": "42a4778f-bfd0-4638-8972-f366cd6b0a4f"
      },
      "outputs": [],
      "source": [
        "# 학습 데이터의 class, image path, target에 대한 정보가 들어있는 csv파일을 읽기.\n",
        "train_info = pd.read_csv(traindata_info_file)\n",
        "\n",
        "# 총 class의 수를 측정.\n",
        "num_classes = len(train_info['target'].unique())\n",
        "\n",
        "# 각 class별로 8:2의 비율이 되도록 학습과 검증 데이터를 분리.\n",
        "train_df, val_df = train_test_split(\n",
        "    train_info,\n",
        "    test_size=0.2,\n",
        "    stratify=train_info['target'],\n",
        "    random_state=14\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "5f5dc8f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10. 학습 데이터에 대한 증강 이미지 생성 및 저장\n",
        "# 증강된 이미지를 저장할 디렉토리 생성\n",
        "augmented_data_dir = \"data/sketch/augmented_train\"\n",
        "os.makedirs(augmented_data_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "97a5a6d7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "증강된 이미지 생성 및 저장 중...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12016/12016 [02:45<00:00, 72.59it/s]\n"
          ]
        }
      ],
      "source": [
        "# 증강된 이미지 경로와 레이블을 저장할 리스트\n",
        "augmented_image_paths = []\n",
        "augmented_targets = []\n",
        "\n",
        "transformations = [\n",
        "    ('original', A.Compose([A.NoOp()])),  # 원본 이미지 (변환 없음)\n",
        "    ('h_flip', A.Compose([A.HorizontalFlip(p=1.0)])),  # 좌우 뒤집기\n",
        "    ('v_flip', A.Compose([A.VerticalFlip(p=1.0)])),  # 상하 뒤집기\n",
        "    ('rotate_30', A.Compose([A.Rotate(limit=30, p=1.0)])),  # 30도 회전\n",
        "    ('resized_crop', A.Compose([A.RandomResizedCrop(height=224, width=224, scale=(0.8, 1.0), p=1.0)])),  # 랜덤 크롭\n",
        "]\n",
        "\n",
        "print(\"증강된 이미지 생성 및 저장 중...\")\n",
        "for idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
        "    image_path = row['image_path']\n",
        "    target = row['target']\n",
        "\n",
        "    # 원본 이미지 읽기\n",
        "    original_img_path = os.path.join(traindata_dir, image_path)\n",
        "    original_img = cv2.imread(original_img_path)\n",
        "    if original_img is None:\n",
        "        print(f\"Failed to read image {original_img_path}\")\n",
        "        continue\n",
        "    original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # 파일 이름에서 슬래시를 언더스코어로 대체\n",
        "    image_filename = image_path.replace('/', '_').replace('\\\\', '_')\n",
        "\n",
        "    # 각 변환을 순차적으로 적용하고 저장\n",
        "    for name, transform in transformations:\n",
        "        try:\n",
        "            if name == 'original':\n",
        "                augmented_img = original_img  # 원본 이미지 그대로\n",
        "            else:\n",
        "                augmented = transform(image=original_img)  # 변환 적용\n",
        "                augmented_img = augmented['image']\n",
        "\n",
        "            # 이미지 저장 경로 및 파일 이름 구성\n",
        "            augmented_img_name = f\"{name}_{image_filename}\"\n",
        "            save_path = os.path.join(augmented_data_dir, augmented_img_name)\n",
        "\n",
        "            # 이미지 저장\n",
        "            save_success = cv2.imwrite(save_path, cv2.cvtColor(augmented_img, cv2.COLOR_RGB2BGR))\n",
        "            if not save_success:\n",
        "                print(f\"Failed to save image: {save_path}\")\n",
        "            else:\n",
        "                augmented_image_paths.append(augmented_img_name)\n",
        "                augmented_targets.append(target)\n",
        "        except Exception as e:\n",
        "            print(f\"Exception occurred while processing image {image_path} with transform {name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "e5b23e3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 증강된 이미지들의 정보로 데이터프레임 생성\n",
        "augmented_train_df = pd.DataFrame({\n",
        "    'image_path': augmented_image_paths,\n",
        "    'target': augmented_targets\n",
        "})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "8f221498",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 이미지 저장\n",
        "save_success = cv2.imwrite(save_path, cv2.cvtColor(augmented_img, cv2.COLOR_RGB2BGR))\n",
        "if not save_success:\n",
        "    print(f\"Failed to save image: {save_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "a0889892-5f63-4dcf-bcab-9ff2659ad28d",
      "metadata": {
        "id": "a0889892-5f63-4dcf-bcab-9ff2659ad28d"
      },
      "outputs": [],
      "source": [
        "# 학습에 사용할 Transform을 선언.\n",
        "transform_selector = TransformSelector(\n",
        "    transform_type = \"torchvision\"\n",
        ")\n",
        "train_transform = transform_selector.get_transform(is_train=True)\n",
        "val_transform = transform_selector.get_transform(is_train=False)\n",
        "\n",
        "# 학습에 사용할 Dataset을 선언.\n",
        "# 학습에 사용할 Dataset 생성\n",
        "train_dataset = CustomDataset(\n",
        "    root_dir=augmented_data_dir,    # 증강된 이미지들이 저장된 디렉토리\n",
        "    info_df=augmented_train_df,     # 증강된 이미지 정보를 가진 데이터프레임\n",
        "    transform=train_transform\n",
        ")\n",
        "val_dataset = CustomDataset(\n",
        "    root_dir=traindata_dir,\n",
        "    info_df=val_df,\n",
        "    transform=val_transform\n",
        ")\n",
        "\n",
        "# 학습에 사용할 DataLoader를 선언.\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "9447ca16-a1ff-4da4-b5a1-4cf239ebcf80",
      "metadata": {
        "id": "9447ca16-a1ff-4da4-b5a1-4cf239ebcf80",
        "outputId": "5ec3fe95-0a3b-4b2a-cdb7-12db2af0c183"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TimmModel(\n",
              "  (model): EfficientVitMsra(\n",
              "    (patch_embed): PatchEmbedding(\n",
              "      (conv1): ConvNorm(\n",
              "        (conv): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu1): ReLU()\n",
              "      (conv2): ConvNorm(\n",
              "        (conv): Conv2d(24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu2): ReLU()\n",
              "      (conv3): ConvNorm(\n",
              "        (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu3): ReLU()\n",
              "      (conv4): ConvNorm(\n",
              "        (conv): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (stages): Sequential(\n",
              "      (0): EfficientVitStage(\n",
              "        (downsample): Identity()\n",
              "        (blocks): Sequential(\n",
              "          (0): EfficientVitBlock(\n",
              "            (dw0): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "                (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (ffn0): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (mixer): ResidualDrop(\n",
              "              (m): LocalWindowAttention(\n",
              "                (attn): CascadedGroupAttention(\n",
              "                  (qkvs): ModuleList(\n",
              "                    (0-2): 3 x ConvNorm(\n",
              "                      (conv): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                      (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (dws): ModuleList(\n",
              "                    (0): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                    (1): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                    (2): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (proj): Sequential(\n",
              "                    (0): ReLU()\n",
              "                    (1): ConvNorm(\n",
              "                      (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (dw1): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "                (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (ffn1): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): EfficientVitStage(\n",
              "        (downsample): Sequential(\n",
              "          (res1): Sequential(\n",
              "            (0): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "                (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (1): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (patchmerge): PatchMerging(\n",
              "            (conv1): ConvNorm(\n",
              "              (conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act): ReLU()\n",
              "            (conv2): ConvNorm(\n",
              "              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=768, bias=False)\n",
              "              (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (se): SEModule(\n",
              "              (fc1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (bn): Identity()\n",
              "              (act): ReLU(inplace=True)\n",
              "              (fc2): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv3): ConvNorm(\n",
              "              (conv): Conv2d(768, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (res2): Sequential(\n",
              "            (0): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
              "                (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (1): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(288, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(576, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (blocks): Sequential(\n",
              "          (0): EfficientVitBlock(\n",
              "            (dw0): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
              "                (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (ffn0): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(288, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(576, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (mixer): ResidualDrop(\n",
              "              (m): LocalWindowAttention(\n",
              "                (attn): CascadedGroupAttention(\n",
              "                  (qkvs): ModuleList(\n",
              "                    (0-2): 3 x ConvNorm(\n",
              "                      (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (dws): ModuleList(\n",
              "                    (0): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                    (1): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                    (2): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (proj): Sequential(\n",
              "                    (0): ReLU()\n",
              "                    (1): ConvNorm(\n",
              "                      (conv): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                      (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (dw1): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
              "                (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (ffn1): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(288, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(576, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1): EfficientVitBlock(\n",
              "            (dw0): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
              "                (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (ffn0): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(288, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(576, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (mixer): ResidualDrop(\n",
              "              (m): LocalWindowAttention(\n",
              "                (attn): CascadedGroupAttention(\n",
              "                  (qkvs): ModuleList(\n",
              "                    (0-2): 3 x ConvNorm(\n",
              "                      (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (dws): ModuleList(\n",
              "                    (0): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                    (1): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                    (2): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (proj): Sequential(\n",
              "                    (0): ReLU()\n",
              "                    (1): ConvNorm(\n",
              "                      (conv): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                      (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (dw1): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
              "                (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (ffn1): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(288, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(576, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (2): EfficientVitBlock(\n",
              "            (dw0): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
              "                (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (ffn0): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(288, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(576, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (mixer): ResidualDrop(\n",
              "              (m): LocalWindowAttention(\n",
              "                (attn): CascadedGroupAttention(\n",
              "                  (qkvs): ModuleList(\n",
              "                    (0-2): 3 x ConvNorm(\n",
              "                      (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (dws): ModuleList(\n",
              "                    (0): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                    (1): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                    (2): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (proj): Sequential(\n",
              "                    (0): ReLU()\n",
              "                    (1): ConvNorm(\n",
              "                      (conv): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                      (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (dw1): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
              "                (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (ffn1): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(288, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(576, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): EfficientVitStage(\n",
              "        (downsample): Sequential(\n",
              "          (res1): Sequential(\n",
              "            (0): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
              "                (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (1): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(288, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(576, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (patchmerge): PatchMerging(\n",
              "            (conv1): ConvNorm(\n",
              "              (conv): Conv2d(288, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act): ReLU()\n",
              "            (conv2): ConvNorm(\n",
              "              (conv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1152, bias=False)\n",
              "              (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (se): SEModule(\n",
              "              (fc1): Conv2d(1152, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (bn): Identity()\n",
              "              (act): ReLU(inplace=True)\n",
              "              (fc2): Conv2d(288, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv3): ConvNorm(\n",
              "              (conv): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (res2): Sequential(\n",
              "            (0): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "                (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (1): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (blocks): Sequential(\n",
              "          (0): EfficientVitBlock(\n",
              "            (dw0): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "                (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (ffn0): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (mixer): ResidualDrop(\n",
              "              (m): LocalWindowAttention(\n",
              "                (attn): CascadedGroupAttention(\n",
              "                  (qkvs): ModuleList(\n",
              "                    (0-3): 4 x ConvNorm(\n",
              "                      (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (dws): ModuleList(\n",
              "                    (0): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                    (1): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                    (2-3): 2 x ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (proj): Sequential(\n",
              "                    (0): ReLU()\n",
              "                    (1): ConvNorm(\n",
              "                      (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (dw1): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "                (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (ffn1): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1): EfficientVitBlock(\n",
              "            (dw0): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "                (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (ffn0): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (mixer): ResidualDrop(\n",
              "              (m): LocalWindowAttention(\n",
              "                (attn): CascadedGroupAttention(\n",
              "                  (qkvs): ModuleList(\n",
              "                    (0-3): 4 x ConvNorm(\n",
              "                      (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (dws): ModuleList(\n",
              "                    (0): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                    (1): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                    (2-3): 2 x ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (proj): Sequential(\n",
              "                    (0): ReLU()\n",
              "                    (1): ConvNorm(\n",
              "                      (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (dw1): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "                (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (ffn1): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (2): EfficientVitBlock(\n",
              "            (dw0): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "                (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (ffn0): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (mixer): ResidualDrop(\n",
              "              (m): LocalWindowAttention(\n",
              "                (attn): CascadedGroupAttention(\n",
              "                  (qkvs): ModuleList(\n",
              "                    (0-3): 4 x ConvNorm(\n",
              "                      (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (dws): ModuleList(\n",
              "                    (0): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                    (1): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                    (2-3): 2 x ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (proj): Sequential(\n",
              "                    (0): ReLU()\n",
              "                    (1): ConvNorm(\n",
              "                      (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (dw1): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "                (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (ffn1): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (3): EfficientVitBlock(\n",
              "            (dw0): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "                (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (ffn0): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (mixer): ResidualDrop(\n",
              "              (m): LocalWindowAttention(\n",
              "                (attn): CascadedGroupAttention(\n",
              "                  (qkvs): ModuleList(\n",
              "                    (0-3): 4 x ConvNorm(\n",
              "                      (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (dws): ModuleList(\n",
              "                    (0): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                    (1): ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                    (2-3): 2 x ConvNorm(\n",
              "                      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
              "                      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (proj): Sequential(\n",
              "                    (0): ReLU()\n",
              "                    (1): ConvNorm(\n",
              "                      (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (dw1): ResidualDrop(\n",
              "              (m): ConvNorm(\n",
              "                (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "                (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (ffn1): ResidualDrop(\n",
              "              (m): ConvMlp(\n",
              "                (pw1): ConvNorm(\n",
              "                  (conv): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (act): ReLU()\n",
              "                (pw2): ConvNorm(\n",
              "                  (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
              "    (head): NormLinear(\n",
              "      (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop): Dropout(p=0.0, inplace=False)\n",
              "      (linear): Linear(in_features=384, out_features=500, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 학습에 사용할 Model을 선언.\n",
        "model_selector = ModelSelector(\n",
        "    model_type='timm',\n",
        "    num_classes=num_classes,\n",
        "    model_name='efficientvit_m5',\n",
        "    pretrained=True\n",
        ")\n",
        "model = model_selector.get_model()\n",
        "\n",
        "# 선언된 모델을 학습에 사용할 장비로 셋팅.\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba076ee2",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "3c32f36d-4031-4cf4-8914-6e01d8861837",
      "metadata": {
        "id": "3c32f36d-4031-4cf4-8914-6e01d8861837"
      },
      "outputs": [],
      "source": [
        "# Optimizer Configuration\n",
        "max_lr = 1e-4  # CoatNet 모델에 적합한 낮은 학습률\n",
        "weight_decay = 1e-5  # 과도한 정규화 방지를 위한 감소된 weight decay\n",
        "\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=0.0,  # OneCycleLR에 의해 설정될 예정\n",
        "    weight_decay=weight_decay\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "a2bd831e-39a7-4f10-a3a5-aefde280fa89",
      "metadata": {
        "id": "a2bd831e-39a7-4f10-a3a5-aefde280fa89"
      },
      "outputs": [],
      "source": [
        "# Training Configuration\n",
        "epochs = 20  # 모델 수렴을 위한 충분한 에포크 수\n",
        "steps_per_epoch = len(train_loader)\n",
        "\n",
        "# Scheduler Configuration\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=max_lr,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    pct_start=0.1,  # 10%의 학습 단계에서 학습률 증가\n",
        "    anneal_strategy='cos',  # 코사인 곡선에 따라 학습률 감소\n",
        "    cycle_momentum=False,  # AdamW에 대해 모멘텀 조정 비활성화\n",
        "    div_factor=25.0,  # 초기 학습률 = max_lr / div_factor\n",
        "    final_div_factor=10000.0  # 최종 학습률 = 초기 학습률 / final_div_factor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "d17e2556-efcf-42c6-9d73-dc27813e7dff",
      "metadata": {
        "id": "d17e2556-efcf-42c6-9d73-dc27813e7dff"
      },
      "outputs": [],
      "source": [
        "# 앞서 선언한 필요 class와 변수들을 조합해, 학습을 진행할 Trainer를 선언.\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    device=device,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    loss_fn=loss_fn,\n",
        "    epochs=epochs,\n",
        "    result_path=save_result_path\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "0ac1110e-558f-4170-ad8d-42ac8eb5c3c4",
      "metadata": {
        "id": "0ac1110e-558f-4170-ad8d-42ac8eb5c3c4",
        "outputId": "46a3bdf2-07aa-48ca-919d-07a63f896184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 6.1572, Validation Loss: 6.1019, Validation Accuracy: 14.6090\n",
            "\n",
            "Save 0epoch result. Loss = 6.1019\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Train Loss: 4.8397, Validation Loss: 3.1903, Validation Accuracy: 53.1448\n",
            "\n",
            "Save 1epoch result. Loss = 3.1903\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Train Loss: 2.2826, Validation Loss: 2.0416, Validation Accuracy: 71.9800\n",
            "\n",
            "Save 2epoch result. Loss = 2.0416\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Train Loss: 1.4486, Validation Loss: 1.8251, Validation Accuracy: 76.7055\n",
            "\n",
            "Save 3epoch result. Loss = 1.8251\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5, Train Loss: 1.2182, Validation Loss: 1.8009, Validation Accuracy: 77.5042\n",
            "\n",
            "Save 4epoch result. Loss = 1.8009\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6, Train Loss: 1.1271, Validation Loss: 1.7749, Validation Accuracy: 78.3361\n",
            "\n",
            "Save 5epoch result. Loss = 1.7749\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                       \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[80], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 모델 학습.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[47], line 104\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[47], line 57\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     55\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[1;32m     58\u001b[0m     images, targets \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), targets\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/cv12-tBk8zQTN-py3.10/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/cv12-tBk8zQTN-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/cv12-tBk8zQTN-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/cv12-tBk8zQTN-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/cv12-tBk8zQTN-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[0;32mIn[38], line 25\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m], torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# 주어진 인덱스에 해당하는 이미지를 로드하고 변환을 적용한 후, 이미지와 레이블을 반환합니다.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[index])  \u001b[38;5;66;03m# 이미지 경로 조합\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMREAD_COLOR\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 이미지를 BGR 컬러 포맷의 numpy array로 읽어옵니다.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)  \u001b[38;5;66;03m# BGR 포맷을 RGB 포맷으로 변환합니다.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)  \u001b[38;5;66;03m# 설정된 이미지 변환을 적용합니다.\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 모델 학습.\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11087088-9b1f-4f7d-8eb5-72008cc88a50",
      "metadata": {
        "id": "11087088-9b1f-4f7d-8eb5-72008cc88a50"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "6bf92c3c-7b38-4f89-af2a-5dfbabf51926",
      "metadata": {
        "id": "6bf92c3c-7b38-4f89-af2a-5dfbabf51926"
      },
      "outputs": [],
      "source": [
        "# 모델 추론을 위한 함수\n",
        "def inference(\n",
        "    model: nn.Module,\n",
        "    device: torch.device,\n",
        "    test_loader: DataLoader\n",
        "):\n",
        "    # 모델을 평가 모드로 설정\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    with torch.no_grad():  # Gradient 계산을 비활성화\n",
        "        for images in tqdm(test_loader):\n",
        "            # 데이터를 같은 장치로 이동\n",
        "            images = images.to(device)\n",
        "\n",
        "            # 모델을 통해 예측 수행\n",
        "            logits = model(images)\n",
        "            logits = F.softmax(logits, dim=1)\n",
        "            preds = logits.argmax(dim=1)\n",
        "\n",
        "            # 예측 결과 저장\n",
        "            predictions.extend(preds.cpu().detach().numpy())  # 결과를 CPU로 옮기고 리스트에 추가\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "b407c24c-785d-4ffc-b17b-84ae7dc4ecae",
      "metadata": {
        "id": "b407c24c-785d-4ffc-b17b-84ae7dc4ecae"
      },
      "outputs": [],
      "source": [
        "# 추론 데이터의 경로와 정보를 가진 파일의 경로를 설정.\n",
        "testdata_dir = \"data/sketch/test\"\n",
        "testdata_info_file = \"data/sketch/test.csv\"\n",
        "# save_result_path = \"result\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "cbb89c12-3b5d-4647-a8c2-83650dce6281",
      "metadata": {
        "id": "cbb89c12-3b5d-4647-a8c2-83650dce6281"
      },
      "outputs": [],
      "source": [
        "# 추론 데이터의 class, image path, target에 대한 정보가 들어있는 csv파일을 읽기.\n",
        "test_info = pd.read_csv(testdata_info_file)\n",
        "\n",
        "# 총 class 수.\n",
        "# num_classes = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e512b732",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "ecec8773-6045-401e-b307-0a9758374c4c",
      "metadata": {
        "id": "ecec8773-6045-401e-b307-0a9758374c4c"
      },
      "outputs": [],
      "source": [
        "# 추론에 사용할 Transform을 선언.\n",
        "# transform_selector = TransformSelector(\n",
        "#     transform_type = \"torchvision\"\n",
        "# )\n",
        "test_transform = transform_selector.get_transform(is_train=False)\n",
        "\n",
        "# 추론에 사용할 Dataset을 선언.\n",
        "test_dataset = CustomDataset(\n",
        "    root_dir=testdata_dir,\n",
        "    info_df=test_info,\n",
        "    transform=test_transform,\n",
        "    is_inference=True\n",
        ")\n",
        "\n",
        "# 추론에 사용할 DataLoader를 선언.\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    drop_last=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "99cc06c1-ce65-476b-8d5b-b8025fcde443",
      "metadata": {
        "id": "99cc06c1-ce65-476b-8d5b-b8025fcde443"
      },
      "outputs": [],
      "source": [
        "# 추론에 사용할 장비를 선택.\n",
        "# torch라이브러리에서 gpu를 인식할 경우, cuda로 설정.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 추론에 사용할 Model을 선언.\n",
        "model_selector = ModelSelector(\n",
        "    model_type='timm',\n",
        "    num_classes=num_classes,\n",
        "    model_name='efficientvit_m5',\n",
        "    pretrained=False\n",
        ")\n",
        "model = model_selector.get_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "317b966a-254c-4ac6-b9b4-1d39f59d524a",
      "metadata": {
        "id": "317b966a-254c-4ac6-b9b4-1d39f59d524a",
        "outputId": "1f6f5c6c-5cf3-431c-d53b-8ef720693f9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_833469/1021875342.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  torch.load(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# best epoch 모델을 불러오기.\n",
        "model.load_state_dict(\n",
        "    torch.load(\n",
        "        os.path.join(save_result_path, \"best_model.pt\"),\n",
        "        map_location='cpu'\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "514852af-f338-4b27-a5a5-8b65406a8023",
      "metadata": {
        "id": "514852af-f338-4b27-a5a5-8b65406a8023",
        "outputId": "0e10edb5-f998-4676-c6eb-92e4fb377430"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/313 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [01:31<00:00,  3.40it/s]\n"
          ]
        }
      ],
      "source": [
        "# predictions를 CSV에 저장할 때 형식을 맞춰서 저장\n",
        "# 테스트 함수 호출\n",
        "predictions = inference(\n",
        "    model=model,\n",
        "    device=device,\n",
        "    test_loader=test_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "cc96c889-2423-42b2-8c3c-4b1d364ece71",
      "metadata": {
        "id": "cc96c889-2423-42b2-8c3c-4b1d364ece71",
        "outputId": "5050fdb8-d570-4bfd-defd-bb85586402fd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>image_path</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.JPEG</td>\n",
              "      <td>328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.JPEG</td>\n",
              "      <td>414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2.JPEG</td>\n",
              "      <td>143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3.JPEG</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4.JPEG</td>\n",
              "      <td>388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10009</th>\n",
              "      <td>10009</td>\n",
              "      <td>10009.JPEG</td>\n",
              "      <td>235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10010</th>\n",
              "      <td>10010</td>\n",
              "      <td>10010.JPEG</td>\n",
              "      <td>107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10011</th>\n",
              "      <td>10011</td>\n",
              "      <td>10011.JPEG</td>\n",
              "      <td>231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10012</th>\n",
              "      <td>10012</td>\n",
              "      <td>10012.JPEG</td>\n",
              "      <td>277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10013</th>\n",
              "      <td>10013</td>\n",
              "      <td>10013.JPEG</td>\n",
              "      <td>210</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10014 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          ID  image_path  target\n",
              "0          0      0.JPEG     328\n",
              "1          1      1.JPEG     414\n",
              "2          2      2.JPEG     143\n",
              "3          3      3.JPEG      17\n",
              "4          4      4.JPEG     388\n",
              "...      ...         ...     ...\n",
              "10009  10009  10009.JPEG     235\n",
              "10010  10010  10010.JPEG     107\n",
              "10011  10011  10011.JPEG     231\n",
              "10012  10012  10012.JPEG     277\n",
              "10013  10013  10013.JPEG     210\n",
              "\n",
              "[10014 rows x 3 columns]"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 모든 클래스에 대한 예측 결과를 하나의 문자열로 합침\n",
        "test_info['target'] = predictions\n",
        "test_info = test_info.reset_index().rename(columns={\"index\": \"ID\"})\n",
        "test_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "c4efd2f6-d74a-491b-a7b1-fd7cf96f45a4",
      "metadata": {
        "id": "c4efd2f6-d74a-491b-a7b1-fd7cf96f45a4"
      },
      "outputs": [],
      "source": [
        "# DataFrame 저장\n",
        "test_info.to_csv(\"efficientvit_m5_val.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac79e3df-e5c3-4a49-b37e-0dea1300c317",
      "metadata": {
        "id": "ac79e3df-e5c3-4a49-b37e-0dea1300c317"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
