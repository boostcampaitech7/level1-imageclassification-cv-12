data_module: "src.data.custom_data_module.data_module.DataModule"
model_module: "src.plmodules.EfficienNetB7_module.BaseModule"

data_config_path: "/data/ephemeral/home/JSM_Secret_Code/configs/data_configs/data_module.yaml"
augmentation_config_path: "/data/ephemeral/home/JSM_Secret_Code/configs/augmentation_configs/data_augmentation.yaml"

model:
  lr: 0.001  # EfficientNet-B7은 학습률에 민감하므로 낮게 설정

optimizer:
  name: AdamW  # EfficientNet-B7은 AdamW 옵티마이저와 잘 맞음
  params:
    lr: 0.001  # 초기 학습률
    weight_decay: 0.01  # Weight Decay를 통한 Regularization 추가

scheduler:
  name: OneCycleLR
  params:
    max_lr: 0.004  # EfficientNet-B7에 맞춰 max_lr을 더 낮춤
    epochs: 50  # 총 학습 에포크 수
    steps_per_epoch: 313  # 에포크당 스텝 수 (데이터셋 크기에 따라 설정)
    pct_start: 0.3  # 학습률이 증가하는 비율
    anneal_strategy: 'cos'  # 코사인 방식으로 학습률 감소
    div_factor: 10.0  # 초기 학습률은 max_lr / div_factor
    final_div_factor: 10000.0  # 최소 학습률 설정

trainer:
  max_epochs: 50  # EfficientNet-B7의 성능 향상을 위해 학습 에포크 수 늘림
  accelerator: gpu
  devices: 1
  precision: 16  # Mixed Precision 사용

callbacks:
  model_checkpoint:
    monitor: val_loss
    save_top_k: 3  # 가장 성능이 좋은 모델 3개 저장
    mode: min
  early_stopping:
    monitor: val_loss
    patience: 6  # 수렴 시간이 길 수 있어 patience 늘림
    mode: min

seed: 14  # 시드 값 유지
use_kfold: false