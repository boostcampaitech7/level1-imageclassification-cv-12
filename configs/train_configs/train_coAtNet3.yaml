data_module: "src.data.custom_data_module.data_module.DataModule"
model_module: "src.plmodules.CoAtNet3_module.BaseModule"

data_config_path: "/data/ephemeral/home/JSM_Secret_Code/configs/data_configs/data_module.yaml"
augmentation_config_path: "/data/ephemeral/home/JSM_Secret_Code/configs/augmentation_configs/data_augmentation.yaml"

model:
  lr: 0.0005  # 학습률을 더 낮게 설정하여 천천히 학습

optimizer:
  name: AdamW  # AdamW 옵티마이저는 잘 맞음
  params:
    lr: 0.0005  # 초기 학습률을 더 낮게 설정
    weight_decay: 0.01  # Weight Decay를 통한 Regularization 추가

scheduler:
  name: OneCycleLR
  params:
    max_lr: 0.003  # max_lr을 더 낮게 설정
    epochs: 50  # 학습 에포크 수를 늘림
    steps_per_epoch: 313  # 에포크당 스텝 수
    pct_start: 0.4  # 학습률이 증가하는 기간을 더 길게 설정
    anneal_strategy: 'cos'  # 코사인 방식으로 학습률 감소
    div_factor: 15.0  # 초기 학습률을 더 낮게 시작
    final_div_factor: 10000.0  # 최소 학습률 설정

trainer:
  max_epochs: 50  # 더 긴 학습 에포크로 설정하여 천천히 학습
  accelerator: gpu
  devices: 1
  precision: 16  # Mixed Precision으로 메모리 효율 증가

callbacks:
  model_checkpoint:
    monitor: val_loss
    save_top_k: 3  # 성능이 좋은 모델 3개 저장
    mode: min
  early_stopping:
    monitor: val_loss
    patience: 4
    mode: min

seed: 14  # 시드 값 유지
use_kfold: false