data_module: "src.data.custom_data_module.data_module.DataModule"
model_module: "src.plmodules.EfficientNetV2_module.BaseModule"

data_config_path: "/data/ephemeral/home/JSM_Secret_Code/configs/data_configs/data_module.yaml"
augmentation_config_path: "/data/ephemeral/home/JSM_Secret_Code/configs/augmentation_configs/data_augmentation.yaml"

model:
  lr: 0.0005  # EfficientNetV2는 학습률에 더 민감하므로 낮은 값으로 설정

optimizer:
  name: AdamW  # AdamW는 EfficientNetV2와 잘 맞음
  params:
    lr: 0.0005  # 학습률을 낮게 설정하여 안정적인 학습 보장
    weight_decay: 0.02  # Weight Decay를 늘려서 Regularization 효과 극대화

scheduler:
  name: OneCycleLR
  params:
    max_lr: 0.003  # EfficientNetV2는 안정성을 위해 낮은 최대 학습률 설정
    epochs: 40  # EfficientNetV2는 짧은 학습으로도 효과가 좋을 수 있어 에포크 수 줄임
    steps_per_epoch: 313  # 데이터셋 크기에 맞춘 스텝 수 (수동으로 조정 필요)
    pct_start: 0.2  # 학습률이 천천히 증가하도록 비율 설정
    anneal_strategy: 'linear'  # Linear 방식으로 학습률 감소
    div_factor: 25.0  # 초기 학습률은 max_lr / div_factor
    final_div_factor: 10000.0  # 최소 학습률 설정

trainer:
  max_epochs: 50  # EfficientNetV2는 빠르게 수렴하므로 40 에포크로 제한
  accelerator: gpu
  devices: 1
  precision: 16  # Mixed Precision으로 메모리와 속도 최적화

callbacks:
  model_checkpoint:
    monitor: val_loss
    save_top_k: 3  # 가장 성능이 좋은 모델 3개 저장
    mode: min
  early_stopping:
    monitor: val_loss
    patience: 5  # 수렴 시간이 길어질 경우를 대비해 patience를 5로 설정
    mode: min

seed: 14  # 시드 값 유지
use_kfold: false  # EfficientNetV2의 빠른 수렴을 고려해 K-Fold 사용 안 함